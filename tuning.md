

## Tuning
* [CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models](https://arxiv.org/abs/2307.07705). Weilin Zhao, Yuxiang Huang, Xu Han, Zhiyuan Liu, Zhengyan Zhang, Maosong Sun. [[Paper]](https://arxiv.org/abs/2307.07705)
* [![Star](https://img.shields.io/github/stars/liziniu/ReMax.svg?style=social&label=Star)](https://github.com/liziniu/ReMax) [ReMax: A Simple, Effective, and Efficient Method for Aligning Large Language Models](https://arxiv.org/abs/2310.10505). Ziniu Li, Tian Xu, Yushun Zhang, Yang Yu, Ruoyu Sun, Zhi-Quan Luo. [[Paper]](https://arxiv.org/abs/2310.10505)[[Github]](https://github.com/liziniu/ReMax)
* [TRANSOM: An Efficient Fault-Tolerant System for Training LLMs](https://arxiv.org/abs/2310.10046). Baodong Wu, Lei Xia, Qingping Li, Kangyu Li, Xu Chen, Yongqiang Guo, Tieyao Xiang, Yuheng Chen, Shigang Li. [[Paper]](https://arxiv.org/abs/2310.10046)
* [DEFT: Data Efficient Fine-Tuning for Large Language Models via Unsupervised Core-Set Selection](https://arxiv.org/abs/2310.16776). Devleena Das, Vivek Khetan. [[Paper]](https://arxiv.org/abs/2310.16776)
* [![Star](https://img.shields.io/github/stars/yangjianxin1/LongQLoRA.svg?style=social&label=Star)](https://github.com/yangjianxin1/LongQLoRA) [LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models](https://arxiv.org/abs/2311.04879). Jianxin Yang. [[Paper]](https://arxiv.org/abs/2311.04879)[[Github]](https://github.com/yangjianxin1/LongQLoRA)
* [![Star](https://img.shields.io/github/stars/ist-daslab/sparsefinetuning.svg?style=social&label=Star)](https://github.com/ist-daslab/sparsefinetuning) [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/abs/2310.06927v2). Eldar Kurtic, Denis Kuznedelev, Elias Frantar, Michael Goin, Dan Alistarh. [[Paper]](https://arxiv.org/abs/2310.06927v2)[[Github]](https://github.com/ist-daslab/sparsefinetuning)[[Github]](https://github.com/neuralmagic/deepsparse/tree/main/research/mpt)
* [![Star](https://img.shields.io/github/stars/prateeky2806/compeft.svg?style=social&label=Star)](https://github.com/prateeky2806/compeft) [ComPEFT: Compression for Communicating Parameter Efficient Updates via Sparsification and Quantization](https://arxiv.org/abs/2311.13171). Prateek Yadav, Leshem Choshen, Colin Raffel, Mohit Bansal. [[Paper]](https://arxiv.org/abs/2311.13171)[[Github]](https://github.com/prateeky2806/compeft)
* [Towards Better Parameter-Efficient Fine-Tuning for Large Language Models: A Position Paper](https://arxiv.org/abs/2311.13126). Chengyu Wang, Junbing Yan, Wei Zhang, Jun Huang. [[Paper]](https://arxiv.org/abs/2311.13126)
* [![Star](https://img.shields.io/github/stars/ytgui/SPT-proto.svg?style=social&label=Star)](https://github.com/ytgui/SPT-proto) [SPT: Fine-Tuning Transformer-based Language Models Efficiently with Sparsification](https://arxiv.org/abs/2312.10365). Yuntao Gui, Xiao Yan, Peiqi Yin, Han Yang, James Cheng. [[Paper]](https://arxiv.org/abs/2312.10365)[[Github]](https://github.com/ytgui/SPT-proto)
* [![Star](https://img.shields.io/github/stars/nikhil-ghosh-berkeley/loraplus.svg?style=social&label=Star)](https://github.com/nikhil-ghosh-berkeley/loraplus) [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/abs/2402.12354). Soufiane Hayou, Nikhil Ghosh, Bin Yu. [[Paper]](https://arxiv.org/abs/2402.12354)[[Github]](https://github.com/nikhil-ghosh-berkeley/loraplus)
* [Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2402.15751). Yong Liu, Zirui Zhu, Chaoyu Gong, Minhao Cheng, Cho-Jui Hsieh, Yang You. [[Paper]](https://arxiv.org/abs/2402.15751)
* [![Star](https://img.shields.io/github/stars/WooSunghyeon/dropbp.svg?style=social&label=Star)](https://github.com/WooSunghyeon/dropbp) [DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation](https://arxiv.org/abs/2402.17812). Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Sejung Kwon, Dongsuk Jeon, Dongsoo Lee. [[Paper]](https://arxiv.org/abs/2402.17812)[[Github]](https://github.com/WooSunghyeon/dropbp)
* [LoRA-SP: Streamlined Partial Parameter Adaptation for Resource-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/abs/2403.08822). Yichao Wu, Yafei Xiang, Shuning Huo, Yulu Gong, Penghao Liang. [[Paper]](https://arxiv.org/abs/2403.08822)
* [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/abs/2403.14608). Zeyu Han, Chao Gao, Jinyang Liu, Jeff (Jun)Zhang, Sai Qian Zhang. [[Paper]](https://arxiv.org/abs/2403.14608)
* [![Publish](https://img.shields.io/badge/Conference-SemEval'24-blue)]() [![Star](https://img.shields.io/github/stars/ngregoriade/Semeval2024-Shroom.svg?style=social&label=Star)](https://github.com/ngregoriade/Semeval2024-Shroom) [AILS-NTUA at SemEval-2024 Task 6: Efficient model tuning for hallucination detection and analysis](https://arxiv.org/pdf/2404.01210.pdf). Natalia Griogoriadou, Maria Lymperaiou, Giorgos Filandrianos, Giorgos Stamou. [[Paper]](https://arxiv.org/pdf/2404.01210.pdf)[[Github]](https://github.com/ngregoriade/Semeval2024-Shroom)
* [![Star](https://img.shields.io/github/stars/Ledzy/BAdam.svg?style=social&label=Star)](https://github.com/Ledzy/BAdam) [BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models](https://arxiv.org/abs/2404.02827). Qijun Luo, Hengxu Yu, Xiao Li. [[Paper]](https://arxiv.org/abs/2404.02827)[[Github]](https://github.com/Ledzy/BAdam)
* [Intuition-aware Mixture-of-Rank-1-Experts for Parameter Efficient Finetuning](https://arxiv.org/abs/2404.08985). Yijiang Liu, Rongyu Zhang, Huanrui Yang, Kurt Keutzer, Yuan Du, Li Du, Shanghang Zhang. [[Paper]](https://arxiv.org/abs/2404.08985)
* [![Publish](https://img.shields.io/badge/Conference-ICML'24-blue)]() [![Star](https://img.shields.io/github/stars/JingXuTHU/Random-Masking-Finds-Winning-Tickets-for-Parameter-Efficient-Fine-tuning.svg?style=social&label=Star)](https://github.com/JingXuTHU/Random-Masking-Finds-Winning-Tickets-for-Parameter-Efficient-Fine-tuning) [Random Masking Finds Winning Tickets for Parameter Efficient Fine-tuning](https://arxiv.org/abs/2405.02596). Jing Xu, Jingzhao Zhang. [[Paper]](https://arxiv.org/abs/2405.02596)[[Github]](https://github.com/JingXuTHU/Random-Masking-Finds-Winning-Tickets-for-Parameter-Efficient-Fine-tuning)
* [Zeroth-Order Fine-Tuning of LLMs with Extreme Sparsity](https://arxiv.org/abs/2406.02913). Wentao Guo, Jikai Long, Yimeng Zeng, Zirui Liu, Xinyu Yang, Yide Ran, Jacob R. Gardner, Osbert Bastani, Christopher De Sa, Xiaodong Yu, Beidi Chen, Zhaozhuo Xu. [[Paper]](https://arxiv.org/abs/2406.02913)
* [![Publish](https://img.shields.io/badge/Conference-ACL'24%20Findings-blue)]() [![Star](https://img.shields.io/github/stars/gccnlp/Light-PEFT.svg?style=social&label=Star)](https://github.com/gccnlp/Light-PEFT) [Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning](https://arxiv.org/abs/2406.03792). Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, Weiping Wang. [[Paper]](https://arxiv.org/abs/2406.03792)[[Github]](https://github.com/gccnlp/Light-PEFT)
* [BlockLLM: Memory-Efficient Adaptation of LLMs by Selecting and Optimizing the Right Coordinate Blocks](https://arxiv.org/abs/2406.17296). Amrutha Varshini Ramesh, Vignesh Ganapathiraman, Issam H. Laradji, Mark Schmidt. [[Paper]](https://arxiv.org/abs/2406.17296)
* [Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead](https://arxiv.org/abs/2407.00066). Rickard Br√ºel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon. [[Paper]](https://arxiv.org/abs/2407.00066)
* [![Publish](https://img.shields.io/badge/Conference-ICLR'24-blue)]() [![Star](https://img.shields.io/github/stars/LINs-lab/CapaBoost.svg?style=social&label=Star)](https://github.com/LINs-lab/CapaBoost) [Increasing Model Capacity for Free: A Simple Strategy for Parameter Efficient Fine-tuning](https://arxiv.org/abs/2407.01320). Haobo Song, Hao Zhao, Soumajit Majumder, Tao Lin. [[Paper]](https://arxiv.org/abs/2407.01320)[[Github]](https://github.com/LINs-lab/CapaBoost)
* [![Publish](https://img.shields.io/badge/Conference-ACL'24%20PrivateNLP-blue)]() [PocketLLM: Enabling On-Device Fine-Tuning for Personalized LLMs](https://arxiv.org/abs/2407.01031). Dan Peng, Zhihui Fu, Jun Wang. [[Paper]](https://arxiv.org/abs/2407.01031)
* [Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning](https://arxiv.org/abs/2407.05040). Yun-Da Tsai, Mingjie Liu, Haoxing Ren. [[Paper]](https://arxiv.org/abs/2407.05040)