
## Text Compression
| Title & Authors | Introduction | Links |
|:--|  :----: | :---:|
|[![Publish](https://img.shields.io/badge/Conference-ICML'23%20Workshop-blue)]()<br>[EntropyRank: Unsupervised Keyphrase Extraction via Side-Information Optimization for Language Model-based Text Compression](https://arxiv.org/abs/2308.13399) <br> Alexander Tsvetkov. Alon Kipnis |<img width="1002" alt="image" src="figures/EntropyRank.png"> |[Paper](https://arxiv.org/abs/2308.13399)|
|[LLMZip: Lossless Text Compression using Large Language Models](https://arxiv.org/abs/2306.04050) <br> Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-Francois Chamberland, Srinivas Shakkottai |<img width="1002" alt="image" src="figures/LLMZip.png"> |[Paper](https://arxiv.org/abs/2306.04050) \| [Unofficial Github](https://github.com/erika-n/GPTzip)|
|[![Star](https://img.shields.io/github/stars/princeton-nlp/AutoCompressors.svg?style=social&label=Star)](https://github.com/princeton-nlp/AutoCompressors)<br>[Adapting Language Models to Compress Contexts](https://arxiv.org/abs/2305.14788) <br> Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen |<img width="202" alt="image" src="figures/AutoCompressor.png"> |[Github](https://github.com/princeton-nlp/AutoCompressors) <br> [Paper](https://arxiv.org/abs/2305.14788)|
|[In-context Autoencoder for Context Compression in a Large Language Model](https://arxiv.org/abs/2307.06945) <br> Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, Furu Wei |<img width="502" alt="image" src="figures/ICAE.png"> |[Paper](https://arxiv.org/abs/2307.06945)|
|[Nugget 2D: Dynamic Contextual Compression for Scaling Decoder-only Language Model](https://arxiv.org/abs/2310.02409) <br> Guanghui Qin, Corby Rosset, Ethan C. Chau, Nikhil Rao, Benjamin Van Durme |<img width="1002" alt="image" src="figures/nugget2D.png"> |[Paper](https://arxiv.org/abs/2310.02409)|
|[Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning](https://arxiv.org/abs/2312.08901) <br> Xijie Huang, Li Lyna Zhang, Kwang-Ting Cheng, Mao Yang |<img width="1002" alt="image" src="figures/CoT-Max.png"> |[Paper](https://arxiv.org/abs/2312.08901)|
|[ProPD: Dynamic Token Tree Pruning and Generation for LLM Parallel Decoding](https://arxiv.org/abs/2402.13485) <br> Shuzhang Zhong, Zebin Yang, Meng Li, Ruihao Gong, Runsheng Wang, Ru Huang |<img width="1002" alt="image" src="figures/ProPD.png"> |[Paper](https://arxiv.org/abs/2402.13485)|
|[Learning to Compress Prompt in Natural Language Formats](https://arxiv.org/abs/2402.18700) <br> Yu-Neng Chuang, Tianwei Xing, Chia-Yuan Chang, Zirui Liu, Xun Chen, Xia Hu |<img width="1002" alt="image" src="https://arxiv.org/html/2402.18700v1/x1.png"> |[Paper](https://arxiv.org/abs/2402.18700)|
|[LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression](https://arxiv.org/abs/2403.12968) <br> Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Menglin Xia, Xufang Luo, Jue Zhang, Qingwei Lin et al |<img width="1002" alt="image" src="https://arxiv.org/html/2403.12968v1/x1.png"> |[Paper](https://arxiv.org/abs/2403.12968)|
|[![Star](https://img.shields.io/github/stars/3DAgentWorld/Toolkit-for-Prompt-Compression.svg?style=social&label=Star)](https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression)<br>[PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models](https://arxiv.org/abs/2403.17411) <br> Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang |<img width="1002" alt="image" src="https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression/raw/main/imgs/architecture.png"> |[Github](https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression) <br> [Paper](https://arxiv.org/abs/2403.17411)|
|[PROMPT-SAW: Leveraging Relation-Aware Graphs for Textual Prompt Compression](https://arxiv.org/abs/2404.00489) <br> Muhammad Asif Ali, Zhengping Li, Shu Yang, Keyuan Cheng, Yang Cao, Tianhao Huang, Lijie Hu, Lu Yu, Di Wang |<img width="1002" alt="image" src="https://arxiv.org/html/2404.00489v1/x2.png"> |[Paper](https://arxiv.org/abs/2404.00489)|
|[Training LLMs over Neurally Compressed Text](https://arxiv.org/abs/2404.03626) <br> Brian Lester, Jaehoon Lee, Alex Alemi, Jeffrey Pennington, Adam Roberts, Jascha Sohl-Dickstein, Noah Constant |<img width="302" alt="image" src="https://arxiv.org/html/2404.03626v1/x1.png"> |[Paper](https://arxiv.org/abs/2404.03626)|
|[![Publish](https://img.shields.io/badge/Conference-IPCA'24-blue)]()<br>[Adapting LLMs for Efficient Context Processing through Soft Prompt Compression](https://arxiv.org/abs/2404.04997) <br> Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd |<img width="1002" alt="image" src="https://arxiv.org/html/2404.04997v1/extracted/5508155/Fig2.png"> |[Paper](https://arxiv.org/abs/2404.04997)|
|[![Star](https://img.shields.io/github/stars/acr-memorization/.svg?style=social&label=Star)](https://github.com/acr-memorization/)<br>[Rethinking LLM Memorization through the Lens of Adversarial Compression](https://arxiv.org/abs/2404.15146) <br> Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter |<img width="1002" alt="image" src="https://pratyushmaini-acr-viewer.hf.space/file=/tmp/gradio/f054d27283291fa78df9949f26ac605dbea31398/ACR.png"> |[Github](https://github.com/locuslab/acr-memorization/) <br> [Paper](https://arxiv.org/abs/2404.15146) <br> [Project](https://locuslab.github.io/acr-memorization/)|